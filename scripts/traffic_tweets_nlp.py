# -*- coding: utf-8 -*-
"""Traffic tweets NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bDLQez_65WZlmn-8ktOyuNx4EVo1XZN-
"""

import pandas as pd
import numpy as np

df = pd.read_csv('/content/traffic_tweets_2922_anon.csv')
df.head()

df['Tweet'][0]

"""**Preprocessing the Data**"""

!pip install langdetect

from langdetect import detect, DetectorFactory

DetectorFactory.seed = 0

# Function to detect if the tweet is in English
def is_english(tweet):
    try:
        return detect(tweet) == 'en'
    except:
        return False

# Step 2: Remove non-English tweets, keeping the original DataFrame name
df = df[df['Tweet'].apply(is_english)].copy()
df.reset_index(drop=True, inplace=True)

import re

# Function to clean tweet text
def clean_text(text):
    text = re.sub(r"http\S+", "", text)  # Remove URLs
    text = re.sub(r"[^a-zA-Z\s]", "", text)  # Remove special characters and numbers
    text = text.lower().strip()  # Convert to lowercase and remove extra spaces
    return text

df['cleaned_tweet'] = df['Tweet'].apply(clean_text)
df['cleaned_tweet'][0]

"""**Tokenization**"""

from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

# Tokenize the cleaned tweets
df['tokenized_tweet'] = df['cleaned_tweet'].apply(word_tokenize)
df['tokenized_tweet'][0]

"""**Removing Stop Words**"""

from nltk.corpus import stopwords
nltk.download('stopwords')

# Load the default English stopwords list
stop_words = set(stopwords.words('english'))

# Remove 'not' from the stopwords list
stop_words.discard('not')

# Remove stopwords (excluding 'not') from tokenized tweets
df['filtered_tweet'] = df['tokenized_tweet'].apply(lambda x: [word for word in x if word not in stop_words])

df['filtered_tweet'][0]

"""**Lemmatization**"""

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()

# Lemmatize the filtered tweets
df['lemmatized_tweet'] = df['filtered_tweet'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])
# Join lemmatized words back into strings for vectorization
df['final_tweet'] = df['lemmatized_tweet'].apply(lambda x: ' '.join(x))

df['final_tweet']

"""**MODEL TRAINING**"""

pip install vaderSentiment pandas

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Initialize VADER sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

def get_sentiment_label(text):
    score = analyzer.polarity_scores(text)['compound']
    if score >= 0.05:
        return 1  # Positive
    elif score <= -0.05:
        return 0  # Negative
    else:
        return 2  # Neutral

# Apply the function to the tweet column
df['sentiment'] = df['final_tweet'].apply(get_sentiment_label)

df['sentiment']

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import joblib

# Step 1: Vectorize the text data
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['final_tweet'])  # Features (tweets vectorized)
y = df['sentiment']                        # Labels (sentiment)

# Step 2: Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Train a Logistic Regression model (you can choose other models too)
model = LogisticRegression()
model.fit(X_train, y_train)

"""**Model Testing and Results**"""

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# Predict the sentiments on the test set
y_pred = model.predict(X_test)

# Display the classification report
report = classification_report(y_test, y_pred, target_names=["Negative", "Positive", "Neutral"])
print(report)

# Evaluate the model on test data (optional)
accuracy = model.score(X_test, y_test)
print(f"Test Accuracy: {accuracy}")

# Create a DataFrame to compare actual and predicted labels
results_df = pd.DataFrame({
    'Tweet': df.loc[y_test.index, 'final_tweet'],  # Get original tweets based on test set indices
    'Actual Sentiment': y_test,
    'Predicted Sentiment': y_pred
})

# Map numeric sentiments to their string representations
sentiment_map = {0: "Negative", 1: "Positive", 2: "Neutral"}
results_df['Actual Sentiment'] = results_df['Actual Sentiment'].map(sentiment_map)
results_df['Predicted Sentiment'] = results_df['Predicted Sentiment'].map(sentiment_map)

# Display the results
print(results_df.head(10))  # Display the first 10 results

# Optionally, you can visualize the confusion matrix using seaborn
import seaborn as sns
import matplotlib.pyplot as plt

# Display the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", conf_matrix)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Negative", "Positive", "Neutral"],
            yticklabels=["Negative", "Positive", "Neutral"])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

# Save the vectorizer and the model for later use in Streamlit
joblib.dump(vectorizer, 'vectorizer.joblib')
joblib.dump(model, 'sentiment_model.joblib')

!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py

!wget -q -O - ipv4.icanhazip.com

!streamlit run app.py & npx localtunnel --port 8501